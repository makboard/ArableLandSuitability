{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/home/crop_dev/miniconda/envs/crop_env/lib/python3.10/site-packages/torchvision/io/image.py:13: UserWarning: Failed to load image Python extension: '/home/crop_dev/miniconda/envs/crop_env/lib/python3.10/site-packages/torchvision/image.so: undefined symbol: _ZNK3c107SymBool10guard_boolEPKcl'If you don't plan on using image functionality from `torchvision.io`, you can ignore this warning. Otherwise, there might be something wrong with your environment. Did you have `libjpeg` or `libpng` installed before building `torchvision` from source?\n",
      "  warn(\n"
     ]
    }
   ],
   "source": [
    "import os\n",
    "import pickle\n",
    "import random\n",
    "import sys\n",
    "import warnings\n",
    "\n",
    "# sys.path.append(os.path.join(\"../models\"))\n",
    "# import conv_lstm\n",
    "# from conv_lstm import ConvLSTM\n",
    "\n",
    "import torch\n",
    "# from src.model_utils import custom_multiclass_report, CroplandDataModule_LSTM, Crop_LSTM, Crop_PL\n",
    "import torch.nn as nn\n",
    "from torch.utils.data import DataLoader, TensorDataset, Dataset\n",
    "\n",
    "import pytorch_lightning as pl\n",
    "import torchmetrics\n",
    "from pytorch_lightning.callbacks.early_stopping import EarlyStopping\n",
    "from pytorch_lightning.callbacks import LearningRateMonitor"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Read dictionary pkl file\n",
    "with open(os.path.join('..', 'data', 'processed_files', 'pkls', 'X_FR_ROS_lstm.pkl'), \"rb\") as fp:\n",
    "    X = pickle.load(fp)\n",
    "\n",
    "with open(os.path.join('..', 'data', 'processed_files', 'pkls', 'y_FR_ROS_lstm.pkl'), \"rb\") as fp:\n",
    "    y = pickle.load(fp)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [],
   "source": [
    "class CroplandDataset(Dataset):\n",
    "    def __init__(self, X_monthly, X_static, y):\n",
    "        self.X_monthly = X_monthly  \n",
    "        self.X_static = X_static\n",
    "        self.y = y \n",
    "\n",
    "    def __len__(self):\n",
    "        return len(self.y)\n",
    "\n",
    "    def __getitem__(self, idx):\n",
    "        x_monthly = self.X_monthly[idx]\n",
    "        x_static = self.X_static[idx]\n",
    "        target = self.y[idx]\n",
    "        \n",
    "        return (x_monthly, x_static), target"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [],
   "source": [
    "batch_size =512\n",
    "# X_train = torch.Tensor(X['Train'])\n",
    "# y_train = torch.Tensor(y['Train'])\n",
    "# X_test = torch.Tensor(X['Test'])\n",
    "# y_test = torch.Tensor(y['Test'])\n",
    "\n",
    "# train_dataset = TensorDataset((X_train_m, X_train_s),y_train)\n",
    "# test_dataset = TensorDataset(X_test,y_test)\n",
    "\n",
    "trainloader = torch.utils.data.DataLoader(CroplandDataset(torch.FloatTensor(X[\"Train\"][0]),\n",
    "                                                        torch.FloatTensor(X[\"Train\"][1]),\n",
    "                                                        torch.FloatTensor(y[\"Train\"])),\n",
    "                                        batch_size=batch_size,\n",
    "                                        shuffle=True,\n",
    "                                        num_workers=2)\n",
    "\n",
    "testloader = torch.utils.data.DataLoader(CroplandDataset(torch.FloatTensor(X[\"Test\"][0]),\n",
    "                                                        torch.FloatTensor(X[\"Test\"][1]),\n",
    "                                                        torch.FloatTensor(y[\"Test\"])),\n",
    "                                        batch_size=batch_size,\n",
    "                                        shuffle=False,\n",
    "                                        num_workers=2)\n",
    "\n",
    "# testloader = torch.utils.data.DataLoader(test_dataset, batch_size=batch_size,\n",
    "#                                         shuffle=False, num_workers=2)\n",
    "# X_train.size()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [],
   "source": [
    "class ConvLSTMCell(nn.Module):\n",
    "    \"\"\"\n",
    "    Initialize ConvLSTM cell.\n",
    "\n",
    "    Parameters\n",
    "    ----------\n",
    "    input_dim (int): Number of channels of input tensor.\n",
    "    hidden_dim (int): Number of channels of hidden state.\n",
    "    kernel_size (int): Size of the convolutional kernel.\n",
    "    bias (bool): Whether to add the bias.\n",
    "    \"\"\"\n",
    "    \n",
    "    def __init__(self,\n",
    "        input_dim,\n",
    "        hidden_dim,\n",
    "        kernel_size,\n",
    "        bias):\n",
    "\n",
    "        super(ConvLSTMCell, self).__init__()\n",
    "        self.input_dim = input_dim\n",
    "        self.hidden_dim = hidden_dim\n",
    "        self.kernel_size = kernel_size\n",
    "        self.padding = kernel_size[0] // 2\n",
    "        self.bias = bias\n",
    "\n",
    "        self.conv = nn.Conv1d(in_channels=self.input_dim + self.hidden_dim,\n",
    "                            out_channels=4 * self.hidden_dim,\n",
    "                            kernel_size=self.kernel_size,\n",
    "                            padding=self.padding,\n",
    "                            bias=self.bias)\n",
    "\n",
    "    def forward(self, input, cur_state):\n",
    "        h_cur, c_cur = cur_state\n",
    "        combined = torch.cat([input, h_cur], dim=1)  # concatenate along channel axis\n",
    "\n",
    "        combined_conv = self.conv(combined)\n",
    "        cc_i, cc_f, cc_o, cc_g = torch.split(combined_conv, self.hidden_dim, dim=1)\n",
    "        i = torch.sigmoid(cc_i)\n",
    "        f = torch.sigmoid(cc_f)\n",
    "        o = torch.sigmoid(cc_o)\n",
    "        g = torch.tanh(cc_g)\n",
    "\n",
    "        c_next = f * c_cur + i * g\n",
    "        h_next = o * torch.tanh(c_next)\n",
    "\n",
    "        return h_next, c_next\n",
    "\n",
    "    def init_hidden(self, batch_size, length):\n",
    "        return (torch.zeros(batch_size, self.hidden_dim, length, device=self.conv.weight.device),\n",
    "                torch.zeros(batch_size, self.hidden_dim, length, device=self.conv.weight.device))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [],
   "source": [
    "class CropConvLSTM(nn.Module):\n",
    "\n",
    "    \"\"\"\n",
    "    A PyTorch module implementing a Crop Conv LSTM network.\n",
    "    \n",
    "    Parameters:\n",
    "        input_dim: Number of channels in input\n",
    "        hidden_dim: Number of hidden channels\n",
    "        kernel_size: Size of kernel in convolutions\n",
    "        n_layers: Number of LSTM layers stacked on each other\n",
    "        bias: Bias or no bias in Convolution\n",
    "        return_all_layers: Return the list of computations for all layers\n",
    "\n",
    "    Input:\n",
    "        A tensor of size B, T, C\n",
    "    Output:\n",
    "        A tuple of two lists of length n_layers (or length 1 if return_all_layers is False).\n",
    "            0 - layer_output_list is the list of lists of length T of each output\n",
    "            1 - last_state_list is the list of last states\n",
    "                    each element of the list is a tuple (h, c) for hidden state and memory\n",
    "    \"\"\"\n",
    "\n",
    "    def __init__(self,\n",
    "        input_dim: int,\n",
    "        hidden_dim: int,\n",
    "        kernel_size: tuple,\n",
    "        n_layers: int,\n",
    "        n_classes: int,\n",
    "        input_len_monthly: int,\n",
    "        seq_len: int,\n",
    "        input_len_static: int,\n",
    "        bias: bool=True,\n",
    "        return_all_layers: bool=False\n",
    "        ) -> None:\n",
    "        super(CropConvLSTM, self).__init__()\n",
    "\n",
    "        \n",
    "        \n",
    "        assert (isinstance(kernel_size, tuple) or\n",
    "                (isinstance(kernel_size, list) and all([isinstance(elem, tuple) for elem in kernel_size]))),'`kernel_size` must be tuple or list of tuples' \n",
    "        # self._check_kernel_size_consistency(kernel_size)\n",
    "        \n",
    "        # Make sure that both `kernel_size` and `hidden_dim` are lists having len == n_layers\n",
    "        kernel_size = self._extend_for_multilayer(kernel_size, n_layers)\n",
    "        hidden_dim = self._extend_for_multilayer(hidden_dim, n_layers)\n",
    "        assert len(kernel_size) == len(hidden_dim) == n_layers, 'Inconsistent list length.'\n",
    "\n",
    "        self.input_dim = input_dim\n",
    "        self.hidden_dim = hidden_dim\n",
    "        self.kernel_size = kernel_size\n",
    "        self.n_layers = n_layers\n",
    "        self.bias = bias\n",
    "        self.return_all_layers = return_all_layers\n",
    "        self.n_classes = n_classes\n",
    "        self.input_len_monthly = input_len_monthly\n",
    "        self.seq_len = seq_len\n",
    "        self.input_len_static = input_len_static\n",
    "        \n",
    "        cell_list = []\n",
    "        for i in range(0, self.n_layers):\n",
    "            cur_input_dim = self.input_dim if i == 0 else self.hidden_dim[i - 1]\n",
    "\n",
    "            cell_list.append(ConvLSTMCell(input_dim=cur_input_dim,\n",
    "                                        hidden_dim=self.hidden_dim[i],\n",
    "                                        kernel_size=self.kernel_size[i],\n",
    "                                        bias=self.bias))\n",
    "        self.cell_list = nn.ModuleList(cell_list)\n",
    "        \n",
    "        self.flatten = nn.Flatten()\n",
    "        self.net = nn.Sequential(\n",
    "            nn.Linear(self.hidden_dim[0]*self.seq_len*self.input_len_monthly+self.input_len_static, self.hidden_dim[0]*self.seq_len),\n",
    "            nn.BatchNorm1d(self.hidden_dim[0]*self.seq_len),\n",
    "            nn.ReLU(),\n",
    "            nn.Dropout(0.5),\n",
    "            nn.Linear(self.hidden_dim[0]*self.seq_len, self.seq_len),\n",
    "            nn.BatchNorm1d(self.seq_len),\n",
    "            nn.ReLU(),\n",
    "            nn.Dropout(0.4),\n",
    "            nn.Linear(self.seq_len, self.n_classes)\n",
    "        )\n",
    "        \n",
    "    def forward(self, input, hidden_state=None):\n",
    "        \"\"\"\n",
    "        Args:\n",
    "        input (tuple):  input[0] is a tensor of shape (batch_size, sequence_length, input_size) containing the monthly input sequence.\n",
    "                        input[1] is a tensor of shape (batch_size, input_size) containing the static input sequence.\n",
    "\n",
    "        Returns\n",
    "        -------\n",
    "        last_state_list, layer_output\n",
    "        \"\"\"\n",
    "        input_monthly = input[0][:, None, :, :]  #fictional dimension added. will be used as channnels\n",
    "        input_static = input[1]\n",
    "        b = input_monthly.size()[0]\n",
    "\n",
    "        # Implement stateful ConvLSTM\n",
    "        if hidden_state is not None:\n",
    "            raise NotImplementedError()\n",
    "        else:\n",
    "            hidden_state = self._init_hidden(batch_size=b, length=self.input_len_monthly)\n",
    "\n",
    "        layer_output_list = []\n",
    "        last_state_list = []\n",
    "        cur_layer_input = input_monthly\n",
    "\n",
    "        for layer_idx in range(self.n_layers):\n",
    "\n",
    "            h, c = hidden_state[layer_idx]\n",
    "            output_inner = []\n",
    "            for t in range(self.seq_len):\n",
    "                h, c = self.cell_list[layer_idx](input = cur_layer_input[:, :, t, :],\n",
    "                                                cur_state=[h, c])\n",
    "                output_inner.append(h)\n",
    "\n",
    "            layer_output = torch.stack(output_inner, dim=1)\n",
    "            cur_layer_input = layer_output\n",
    "\n",
    "            layer_output_list.append(layer_output)\n",
    "            last_state_list.append([h, c])\n",
    "\n",
    "        if not self.return_all_layers:\n",
    "            layer_output_list = layer_output_list[-1:]\n",
    "            last_state_list = last_state_list[-1:]\n",
    "\n",
    "        output_monthly = self.flatten(layer_output_list[0])        \n",
    "        output = self.net(torch.cat((output_monthly, input_static), dim=1))\n",
    "\n",
    "        return output\n",
    "\n",
    "    def _init_hidden(self, batch_size, length):\n",
    "        init_states = []\n",
    "        for i in range(self.n_layers):\n",
    "            init_states.append(self.cell_list[i].init_hidden(batch_size, length))\n",
    "        return init_states\n",
    "\n",
    "    # @staticmethod\n",
    "    # def _check_kernel_size_consistency(kernel_size):\n",
    "    #     if not (isinstance(kernel_size, tuple) or\n",
    "    #             (isinstance(kernel_size, list) and all([isinstance(elem, tuple) for elem in kernel_size]))):\n",
    "    #         raise ValueError('`kernel_size` must be tuple or list of tuples')\n",
    "\n",
    "    @staticmethod\n",
    "    def _extend_for_multilayer(param, n_layers):\n",
    "        if not isinstance(param, list):\n",
    "            param = [param] * n_layers\n",
    "        return param\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {},
   "outputs": [],
   "source": [
    "network = CropConvLSTM(\n",
    "    input_dim=1, #fictional dimension. will be used as channnels\n",
    "    hidden_dim=16,\n",
    "    kernel_size=(3,),\n",
    "    n_layers=1,\n",
    "    n_classes = 4,\n",
    "    input_len_monthly = X['Train'][0].shape[2],\n",
    "    seq_len = X['Train'][0].shape[1],\n",
    "    input_len_static = X['Train'][1].shape[1],\n",
    "    bias=False,\n",
    "    return_all_layers=False\n",
    "    )\n",
    "\n",
    "import torch.optim as optim\n",
    "\n",
    "criterion = nn.CrossEntropyLoss()\n",
    "optimizer = optim.SGD(network.parameters(), lr=0.001, momentum=0.9)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Finished Training\n"
     ]
    }
   ],
   "source": [
    "for epoch in range(50):  # loop over the dataset multiple times\n",
    "\n",
    "    running_loss = 0.0\n",
    "    for i, data in enumerate(trainloader, 0):\n",
    "        # get the inputs; data is a list of [inputs, labels]\n",
    "        inputs, labels = data\n",
    "\n",
    "        # zero the parameter gradients\n",
    "        optimizer.zero_grad()\n",
    "        \n",
    "        # forward + backward + optimize\n",
    "        output = network(inputs)\n",
    "        loss = criterion(output, labels)\n",
    "        loss.backward()\n",
    "        optimizer.step()\n",
    "\n",
    "        # print statistics\n",
    "        running_loss += loss.item()\n",
    "        if i % 1000 == 999:    # print every 2000 mini-batches\n",
    "            print(f'[{epoch + 1}, {i + 1:5d}] loss: {running_loss / 2000:.3f}')\n",
    "            running_loss = 0.0\n",
    "\n",
    "# preds = nn.Softmax(output)\n",
    "# f1 = torchmetrics.F1Score(task=\"multiclass\", num_classes=4, average=\"macro\")\n",
    "# f1(preds, labels)\n",
    "print('Finished Training')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "metadata": {},
   "outputs": [],
   "source": [
    "\n",
    "# %%\n",
    "# Save the module to a file\n",
    "model_filename = os.path.join(\"..\", \"results\", \"pickle_models\", \"lstm_pytorch_ROS.pkl\")\n",
    "torch.save(network, model_filename)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 51,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "f1 equals tensor(0.2898)\n",
      "Accuracy for class: 0 is 30.6 %\n"
     ]
    },
    {
     "ename": "ZeroDivisionError",
     "evalue": "float division by zero",
     "output_type": "error",
     "traceback": [
      "\u001b[0;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[0;31mZeroDivisionError\u001b[0m                         Traceback (most recent call last)",
      "\u001b[1;32m/app/ArableLandSuitability/models/conv_lstm_pytorch.ipynb Cell 10\u001b[0m line \u001b[0;36m2\n\u001b[1;32m     <a href='vscode-notebook-cell://attached-container%2B7b22636f6e7461696e65724e616d65223a222f47465341445f4461726961222c2273657474696e6773223a7b22686f7374223a227373683a2f2f31302e31362e38382e3338227d7d/app/ArableLandSuitability/models/conv_lstm_pytorch.ipynb#X11sdnNjb2RlLXJlbW90ZQ%3D%3D?line=22'>23</a>\u001b[0m \u001b[39m# print accuracy for each class\u001b[39;00m\n\u001b[1;32m     <a href='vscode-notebook-cell://attached-container%2B7b22636f6e7461696e65724e616d65223a222f47465341445f4461726961222c2273657474696e6773223a7b22686f7374223a227373683a2f2f31302e31362e38382e3338227d7d/app/ArableLandSuitability/models/conv_lstm_pytorch.ipynb#X11sdnNjb2RlLXJlbW90ZQ%3D%3D?line=23'>24</a>\u001b[0m \u001b[39mfor\u001b[39;00m classname, correct_count \u001b[39min\u001b[39;00m correct_pred\u001b[39m.\u001b[39mitems():\n\u001b[0;32m---> <a href='vscode-notebook-cell://attached-container%2B7b22636f6e7461696e65724e616d65223a222f47465341445f4461726961222c2273657474696e6773223a7b22686f7374223a227373683a2f2f31302e31362e38382e3338227d7d/app/ArableLandSuitability/models/conv_lstm_pytorch.ipynb#X11sdnNjb2RlLXJlbW90ZQ%3D%3D?line=24'>25</a>\u001b[0m     accuracy \u001b[39m=\u001b[39m \u001b[39m100\u001b[39;49m \u001b[39m*\u001b[39;49m \u001b[39mfloat\u001b[39;49m(correct_count) \u001b[39m/\u001b[39;49m total_pred[classname]\n\u001b[1;32m     <a href='vscode-notebook-cell://attached-container%2B7b22636f6e7461696e65724e616d65223a222f47465341445f4461726961222c2273657474696e6773223a7b22686f7374223a227373683a2f2f31302e31362e38382e3338227d7d/app/ArableLandSuitability/models/conv_lstm_pytorch.ipynb#X11sdnNjb2RlLXJlbW90ZQ%3D%3D?line=25'>26</a>\u001b[0m     \u001b[39mprint\u001b[39m(\u001b[39mf\u001b[39m\u001b[39m'\u001b[39m\u001b[39mAccuracy for class: \u001b[39m\u001b[39m{\u001b[39;00mclassname\u001b[39m}\u001b[39;00m\u001b[39m is \u001b[39m\u001b[39m{\u001b[39;00maccuracy\u001b[39m:\u001b[39;00m\u001b[39m.1f\u001b[39m\u001b[39m}\u001b[39;00m\u001b[39m %\u001b[39m\u001b[39m'\u001b[39m)\n",
      "\u001b[0;31mZeroDivisionError\u001b[0m: float division by zero"
     ]
    }
   ],
   "source": [
    "# prepare to count predictions for each class\n",
    "classes = (0,1,2,3)\n",
    "correct_pred = {classname: 0 for classname in classes}\n",
    "total_pred = {classname: 0 for classname in classes}\n",
    "f1 = torchmetrics.F1Score(task = 'multiclass', num_classes = 4, average = 'macro')\n",
    "\n",
    "# again no gradients needed\n",
    "with torch.no_grad():\n",
    "    for data in testloader:\n",
    "        images, labels = data\n",
    "        outputs = network(images)\n",
    "        _, predictions = torch.max(outputs, 1)\n",
    "        labels = torch.argmax(labels, 1)\n",
    "        f1.update(predictions, labels)\n",
    "        # collect the correct predictions for each class\n",
    "        for label, prediction in zip(labels, predictions):\n",
    "            class_label = torch.argmax(label)\n",
    "            if class_label == int(prediction):\n",
    "                correct_pred[classes[class_label]] += 1\n",
    "            total_pred[classes[class_label]] += 1\n",
    "total_f1 = f1.compute()\n",
    "print('f1 equals', total_f1)\n",
    "# print accuracy for each class\n",
    "for classname, correct_count in correct_pred.items():\n",
    "    accuracy = 100 * float(correct_count) / total_pred[classname]\n",
    "    print(f'Accuracy for class: {classname} is {accuracy:.1f} %')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "crop_env",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.10.12"
  },
  "orig_nbformat": 4
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
