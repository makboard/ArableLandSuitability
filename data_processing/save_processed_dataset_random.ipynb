{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [],
   "source": [
    "import os\n",
    "import pickle\n",
    "import sys\n",
    "\n",
    "import copy\n",
    "import random\n",
    "import joblib\n",
    "import numpy as np\n",
    "import pandas as pd\n",
    "import rioxarray as rxr\n",
    "import torch\n",
    "from sklearn.preprocessing import OneHotEncoder\n",
    "from sklearn.model_selection import train_test_split\n",
    "from tqdm.auto import tqdm\n",
    "\n",
    "sys.path.append(\"..\")\n",
    "\n",
    "from sklearn.model_selection import train_test_split\n",
    "from sklearn.preprocessing import MinMaxScaler\n",
    "\n",
    "from src.model_utils import reshape_data, downsample\n",
    "from src.dataprocessing import generate_subsets, generate_blocks"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Paths to data"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [],
   "source": [
    "# defining paths\n",
    "path_to_npys_data = os.path.join(\"..\", \"data\", \"npys_data\")\n",
    "\n",
    "pathTarget = os.path.join(os.path.join(path_to_npys_data, \"target_croplands.npy\"))\n",
    "pathFeatures = os.path.join(path_to_npys_data, \"features_initial_data.npy\")\n",
    "pathMorf = os.path.join(path_to_npys_data, \"features_morf_data.npy\")\n",
    "pathTarget_tif = os.path.join(\"..\", \"data\", \"target\", \"target_croplands.tif\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "##  Data"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Features\n",
    "climate_features = pd.DataFrame.from_dict(\n",
    "    np.load(pathFeatures, allow_pickle=True), orient=\"columns\"\n",
    ")\n",
    "morf_features = pd.DataFrame.from_dict(\n",
    "    np.load(pathMorf, allow_pickle=True), orient=\"columns\"\n",
    ")\n",
    "\n",
    "climate_keys = list(climate_features.keys())\n",
    "morf_keys = list(morf_features.keys())\n",
    "\n",
    "with open(os.path.join(path_to_npys_data, \"climate_keys.pkl\"), \"wb\") as file:\n",
    "    pickle.dump(climate_keys, file)\n",
    "\n",
    "with open(os.path.join(path_to_npys_data, \"morf_keys.pkl\"), \"wb\") as file:\n",
    "    pickle.dump(morf_keys, file)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Target Variable\n",
    "y = pd.DataFrame.from_dict(np.load(pathTarget, allow_pickle=True), orient=\"columns\")\n",
    "y = y[\"Target\"].astype(int)\n",
    "# Set classes 4,5 to 0\n",
    "y = pd.DataFrame({\"target\": np.where(y > 3, 0, y)})"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Data Preparation \n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Train/val/test split using pixels blocks"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [],
   "source": [
    "# combine climate morf and target and then filter to make holdout \n",
    "climate_features.drop(columns=['latitude', 'longitude'], inplace=True)\n",
    "data = pd.concat([climate_features, morf_features, y], axis=1)\n",
    "\n",
    "main_data = data[data['longitude'] <= 100]\n",
    "hold_out = data[(115 <= data['longitude']) & (data['longitude'] <= 135) &\n",
    "                     (42 <= data['latitude']) & (data['latitude'] <= 55)]\n",
    "\n",
    "X_keys = list(data.keys()[:-1])\n",
    "\n",
    "with open(os.path.join(path_to_npys_data, \"X_keys.pkl\"), 'wb') as file:\n",
    "    pickle.dump(X_keys, file)\n",
    "\n",
    "del data"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Reshape features and target dataframes back to its original shape\n",
    "y = main_data.pop(\"target\").to_numpy()\n",
    "X = main_data.values\n",
    "\n",
    "# holdout\n",
    "y_holdout = hold_out.pop(\"target\").to_numpy()\n",
    "X_holdout = hold_out.to_numpy()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Target one hot encoding and Train/test split"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "X_train shape: (21392910, 164)\n",
      "X_val shape: (218295, 164)\n",
      "X_test shape: (218295, 164)\n",
      "X_holdout shape: (3241413, 164)\n"
     ]
    }
   ],
   "source": [
    "y = pd.DataFrame(y, columns=[\"Target\"])\n",
    "# read data and apply one-hot encoding\n",
    "ohe = OneHotEncoder(handle_unknown=\"ignore\", sparse=False).fit(y)\n",
    "y = ohe.transform(y)\n",
    "\n",
    "# Train/test split\n",
    "X_train, X_test, y_train, y_test = train_test_split(\n",
    "    X, y, test_size=0.02, stratify=y, random_state=123\n",
    ")\n",
    "\n",
    "X_test, X_val, y_test, y_val = train_test_split(\n",
    "    X_test, y_test, test_size=0.5, stratify=y_test, random_state=123\n",
    ")\n",
    "\n",
    "# Define scaler based on train set\n",
    "scaler = MinMaxScaler()\n",
    "minmax = scaler.fit(X_train)\n",
    "joblib.dump(minmax, os.path.join(path_to_npys_data, \"scaler_FR_RUS.save\"))\n",
    "\n",
    "# Normalization using minmax scaler\n",
    "X_train = minmax.transform(X_train)\n",
    "X_val = minmax.transform(X_val)\n",
    "X_test = minmax.transform(X_test)\n",
    "X_holdout = minmax.transform(X_holdout)\n",
    "\n",
    "X = dict()\n",
    "y = dict()\n",
    "\n",
    "print(\"X_train shape:\", X_train.shape)\n",
    "print(\"X_val shape:\", X_val.shape)\n",
    "print(\"X_test shape:\", X_test.shape)\n",
    "print(\"X_holdout shape:\", X_holdout.shape)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### RUS ROS sampling for train set"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Initial data:\n",
      "[(0, 17225658), (3, 2691221), (2, 1240911), (1, 235120)]\n",
      "Resampled data:\n",
      "[(0, 2691221), (1, 2691221), (2, 2691221), (3, 2691221)]\n"
     ]
    }
   ],
   "source": [
    "X_train, y_train = downsample(X_train, np.argmax(y_train, 1), oversampling=True)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {},
   "outputs": [],
   "source": [
    "X[\"Train\"] = X_train\n",
    "X[\"Val\"] = X_val\n",
    "X[\"Test\"] = X_test\n",
    "y[\"Train\"] = ohe.transform(pd.DataFrame(y_train))\n",
    "y[\"Val\"] = y_val\n",
    "y[\"Test\"] = y_test"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "metadata": {},
   "outputs": [],
   "source": [
    "# save holdout data \n",
    "with open(os.path.join(\"..\", \"data\", \"processed_files\", \"pkls\", \"X_FR_RUS_holdout.pkl\"), 'wb') as file:\n",
    "    pickle.dump(X_holdout, file)\n",
    "    \n",
    "with open(os.path.join(\"..\", \"data\", \"processed_files\", \"pkls\", \"y_holdout.pkl\"), 'wb') as file:\n",
    "    pickle.dump(y_holdout, file)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "metadata": {},
   "outputs": [],
   "source": [
    "# save dictionary pkl file\n",
    "with open(os.path.join(\"..\", \"data\", \"processed_files\", \"pkls\", \"X_FR_RUS.pkl\"), \"wb\") as fp:\n",
    "    pickle.dump(X, fp)\n",
    "\n",
    "with open(os.path.join(\"..\", \"data\", \"processed_files\", \"pkls\", \"y_FR_RUS.pkl\"), \"wb\") as fp:\n",
    "    pickle.dump(y, fp)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "metadata": {},
   "outputs": [],
   "source": [
    "X_Train_monthly, X_Train_static, monthly_keys, static_keys = reshape_data(pd.DataFrame(X_train, columns=X_keys))\n",
    "X_val_monthly, X_val_static, _, _ = reshape_data(pd.DataFrame(X_val, columns=X_keys))\n",
    "X_test_monthly, X_test_static, _, _ = reshape_data(pd.DataFrame(X_test, columns=X_keys))\n",
    "\n",
    "X[\"Train\"] = X_Train_monthly, X_Train_static\n",
    "X[\"Val\"] = X_val_monthly, X_val_static\n",
    "X[\"Test\"] = X_test_monthly, X_test_static"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "metadata": {},
   "outputs": [],
   "source": [
    "with open(os.path.join(path_to_npys_data, \"monthly_keys.pkl\"), \"wb\") as file:\n",
    "    pickle.dump(monthly_keys, file)\n",
    "\n",
    "with open(os.path.join(path_to_npys_data, \"static_keys.pkl\"), \"wb\") as file:\n",
    "    pickle.dump(static_keys, file)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "metadata": {},
   "outputs": [],
   "source": [
    "with open(os.path.join(\"..\", \"data\", \"processed_files\", \"pkls\", \"X_FR_RUS_lstm.pkl\"), \"wb\") as fp:\n",
    "    pickle.dump(X, fp)\n",
    "\n",
    "with open(os.path.join(\"..\", \"data\", \"processed_files\", \"pkls\", \"y_FR_RUS_lstm.pkl\"), \"wb\") as fp:\n",
    "    pickle.dump(y, fp)"
   ]
  }
 ],
 "metadata": {
  "interpreter": {
   "hash": "7005a85ec43a6b831037bb675e384a439dc84b9bb12a3d7a0cc7bf1b3a3e3cfc"
  },
  "kernelspec": {
   "display_name": "Python 3.10.12 ('crop_env')",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.10.12"
  },
  "orig_nbformat": 4
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
