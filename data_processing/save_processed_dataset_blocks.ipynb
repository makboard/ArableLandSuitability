{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "/home/crop_dev/miniconda/envs/crop_env/lib/python3.10/site-packages/bitsandbytes/libbitsandbytes_cpu.so: undefined symbol: cadam32bit_grad_fp32\n"
     ]
    }
   ],
   "source": [
    "import os\n",
    "import pickle\n",
    "import sys\n",
    "\n",
    "import copy\n",
    "import random\n",
    "import joblib\n",
    "import numpy as np\n",
    "import pandas as pd\n",
    "import rioxarray as rxr\n",
    "import torch\n",
    "from sklearn.preprocessing import OneHotEncoder\n",
    "from sklearn.model_selection import train_test_split\n",
    "from tqdm import tqdm\n",
    "\n",
    "sys.path.append(\"..\")\n",
    "\n",
    "from sklearn.model_selection import train_test_split\n",
    "from sklearn.preprocessing import MinMaxScaler\n",
    "\n",
    "from src.model_utils import reshape_data\n",
    "from src.dataprocessing import generate_subsets, generate_blocks"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Paths to data"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# defining paths\n",
    "path_to_npys_data = os.path.join(\"..\", \"data\", \"npys_data\")\n",
    "\n",
    "pathTarget = os.path.join(os.path.join(path_to_npys_data, \"target_croplands.npy\"))\n",
    "pathFeatures = os.path.join(path_to_npys_data, \"features_initial_data.npy\")\n",
    "pathMorf = os.path.join(path_to_npys_data, \"features_morf_data.npy\")\n",
    "pathTarget_tif = os.path.join(\"..\", \"data\", \"target\", \"target_croplands.tif\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "##  Data"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Features\n",
    "climate_features = pd.DataFrame.from_dict(\n",
    "    np.load(pathFeatures, allow_pickle=True), orient=\"columns\"\n",
    ")\n",
    "morf_features = pd.DataFrame.from_dict(\n",
    "    np.load(pathMorf, allow_pickle=True), orient=\"columns\"\n",
    ")\n",
    "\n",
    "climate_keys = list(climate_features.keys())\n",
    "morf_keys = list(morf_features.keys())\n",
    "\n",
    "with open(os.path.join(path_to_npys_data, \"climate_keys.pkl\"), \"wb\") as file:\n",
    "    pickle.dump(climate_keys, file)\n",
    "\n",
    "with open(os.path.join(path_to_npys_data, \"morf_keys.pkl\"), \"wb\") as file:\n",
    "    pickle.dump(morf_keys, file)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Target Variable\n",
    "y = pd.DataFrame.from_dict(np.load(pathTarget, allow_pickle=True), orient=\"columns\")\n",
    "y = y[\"Target\"].astype(int)\n",
    "# Set classes 4,5 to 0\n",
    "y = pd.DataFrame({\"target\": np.where(y > 3, 0, y)})"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Data Preparation \n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Train/val/test split using pixels blocks"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [],
   "source": [
    "# combine climate morf and target and then filter to make holdout \n",
    "climate_features.drop(columns=['latitude', 'longitude'], inplace=True)\n",
    "data = pd.concat([climate_features, morf_features, y], axis=1)\n",
    "\n",
    "main_data = data[data['longitude'] <= 100]\n",
    "hold_out = data[(115 <= data['longitude']) & (data['longitude'] <= 135) &\n",
    "                     (42 <= data['latitude']) & (data['latitude'] <= 55)]\n",
    "\n",
    "X_keys = list(data.keys()[:-1])\n",
    "\n",
    "with open(os.path.join(path_to_npys_data, \"X_keys.pkl\"), 'wb') as file:\n",
    "    pickle.dump(X_keys, file)\n",
    "\n",
    "del data"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Calculate the total number of samples in each class\n",
    "class_counts = np.unique(main_data[\"target\"], return_counts=True)[1]\n",
    "\n",
    "# Calculate the total number of samples\n",
    "total_samples = sum(class_counts)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "2450 8906\n"
     ]
    }
   ],
   "source": [
    "# get nrows and ncols using preprocessed tif\n",
    "nrows, ncols = (\n",
    "    rxr.open_rasterio(pathTarget_tif)\n",
    "    .squeeze()\n",
    "    .where(rxr.open_rasterio(pathTarget_tif).squeeze()[\"x\"] <= 100, drop=True)\n",
    "    .shape\n",
    ")\n",
    "print(nrows, ncols)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Reshape features and target dataframes back to its original shape\n",
    "y = main_data.pop(\"target\").to_numpy().reshape(nrows, ncols)\n",
    "X = main_data.values.reshape(nrows, ncols, -1)\n",
    "\n",
    "# holdout\n",
    "y_holdout = hold_out.pop(\"target\").to_numpy()\n",
    "X_holdout = hold_out.to_numpy()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "minblocks = np.inf\n",
    "for iter in tqdm(range(100)):\n",
    "    # Generate 200x200 blocks from X and y\n",
    "    X_blocks = generate_blocks(X, block_size=(200, 200))\n",
    "    y_blocks = generate_blocks(y, block_size=(200, 200))\n",
    "    blocks = list(zip(X_blocks, y_blocks))\n",
    "\n",
    "    max_iterations = 10 * len(blocks)\n",
    "\n",
    "    train, val, test = [], [], []\n",
    "    train_distr, val_distr, test_distr, empty = (\n",
    "        {0: 0, 1: 0, 2: 0, 3: 0} for i in range(4)\n",
    "    )\n",
    "\n",
    "    options = [train, val, test]\n",
    "    options_distr = [train_distr, val_distr, test_distr]\n",
    "\n",
    "    options, options_distr, blocks = generate_subsets(\n",
    "        blocks, empty, max_iterations, class_counts, options, options_distr, 0.8, 0.1\n",
    "    )\n",
    "\n",
    "    if len(blocks) < minblocks:\n",
    "        minblocks = len(blocks)\n",
    "        results = copy.deepcopy(options)\n",
    "        results_distr = copy.deepcopy(options_distr)\n",
    "        residual_blocks = copy.deepcopy(blocks)\n",
    "    if minblocks == 0:\n",
    "        break\n",
    "\n",
    "# work with residuals increasing limits\n",
    "max_iterations = 10 * minblocks\n",
    "options, options_distr, blocks = generate_subsets(\n",
    "    residual_blocks,\n",
    "    empty,\n",
    "    max_iterations,\n",
    "    class_counts,\n",
    "    results,\n",
    "    results_distr,\n",
    "    0.85,\n",
    "    0.15,\n",
    ")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Check residuals\n",
    "for i in range(len(blocks)):\n",
    "    print(\"Block \", i)\n",
    "    print(np.unique(residual_blocks[i][1].flatten(), return_counts=True)[1])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Check results distr\n",
    "for s, set in enumerate([\"train\", \"val\", \"test\"]):\n",
    "    print(set, [results_distr[s][i] / class_counts[i] for i in range(4)])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 36,
   "metadata": {},
   "outputs": [],
   "source": [
    "X_train = np.concatenate([block[0].reshape(-1, len(X_keys)) for block in options[0]], axis=0)\n",
    "X_val = np.concatenate([block[0].reshape(-1, len(X_keys)) for block in options[1]], axis=0)\n",
    "X_test = np.concatenate([block[0].reshape(-1, len(X_keys)) for block in options[2]], axis=0)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 44,
   "metadata": {},
   "outputs": [],
   "source": [
    "y_train = np.concatenate([block[1].reshape(-1, 1) for block in options[0]], axis=0)\n",
    "y_val = np.concatenate([block[1].reshape(-1, 1) for block in options[1]], axis=0)\n",
    "y_test = np.concatenate([block[1].reshape(-1, 1) for block in options[2]], axis=0)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Target one hot encoding and Train/test split"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 49,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/opt/conda/envs/crop/lib/python3.11/site-packages/sklearn/preprocessing/_encoders.py:972: FutureWarning: `sparse` was renamed to `sparse_output` in version 1.2 and will be removed in 1.4. `sparse_output` is ignored unless you leave `sparse` to its default value.\n",
      "  warnings.warn(\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "X_train shape: (17436100, 164)\n",
      "X_test shape: (2170000, 164)\n",
      "X_val shape: (2213600, 164)\n"
     ]
    }
   ],
   "source": [
    "# read data and apply one-hot encoding\n",
    "ohe = OneHotEncoder(handle_unknown=\"ignore\", sparse=False).fit(y_train)\n",
    "y_train = ohe.transform(y_train)\n",
    "y_val = ohe.transform(y_val)\n",
    "y_test = ohe.transform(y_test)\n",
    "\n",
    "# Define scaler based on whole dataset\n",
    "scaler = MinMaxScaler()\n",
    "minmax = scaler.fit(X_train)\n",
    "joblib.dump(minmax, os.path.join(path_to_npys_data, \"scaler.save\"))\n",
    "\n",
    "# Normalization using minmax scaler\n",
    "X_train = minmax.transform(X_train)\n",
    "X_val = minmax.transform(X_val)\n",
    "X_test = minmax.transform(X_test)\n",
    "X_holdout = minmax.transform(X_holdout)\n",
    "\n",
    "X = dict()\n",
    "y = dict()\n",
    "\n",
    "print(\"X_train shape:\", X_train.shape)\n",
    "print(\"X_val shape:\", X_val.shape)\n",
    "print(\"X_test shape:\", X_test.shape)\n",
    "print(\"X_holdout shape:\", X_holdout.shape)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 50,
   "metadata": {},
   "outputs": [],
   "source": [
    "X[\"Train\"] = X_train\n",
    "X[\"Val\"] = X_val\n",
    "X[\"Test\"] = X_test\n",
    "y[\"Train\"] = y_train\n",
    "y[\"Val\"] = y_val\n",
    "y[\"Test\"] = y_test"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# save holdout data \n",
    "with open(os.path.join(\"..\", \"data\", \"processed_files\", \"pkls\", \"X_holdout.pkl\"), 'wb') as file:\n",
    "    pickle.dump(X_holdout, file)\n",
    "    \n",
    "with open(os.path.join(\"..\", \"data\", \"processed_files\", \"pkls\", \"y_holdout.pkl\"), 'wb') as file:\n",
    "    pickle.dump(y_holdout, file)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# save dictionary pkl file\n",
    "with open(os.path.join(\"..\", \"data\", \"processed_files\", \"pkls\", \"X.pkl\"), \"wb\") as fp:\n",
    "    pickle.dump(X, fp)\n",
    "\n",
    "with open(os.path.join(\"..\", \"data\", \"processed_files\", \"pkls\", \"y.pkl\"), \"wb\") as fp:\n",
    "    pickle.dump(y, fp)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 22,
   "metadata": {},
   "outputs": [],
   "source": [
    "X[\"Train\"] = reshape_data(pd.DataFrame(X_train, columns=X_keys))\n",
    "X[\"Val\"] = reshape_data(pd.DataFrame(X_val, columns=X_keys))\n",
    "X[\"Test\"] = reshape_data(pd.DataFrame(X_test, columns=X_keys))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 24,
   "metadata": {},
   "outputs": [],
   "source": [
    "with open(os.path.join(\"..\", \"data\", \"processed_files\", \"pkls\", \"X_lstm.pkl\"), \"wb\") as fp:\n",
    "    pickle.dump(X, fp)\n",
    "\n",
    "with open(os.path.join(\"..\", \"data\", \"processed_files\", \"pkls\", \"y_lstm.pkl\"), \"wb\") as fp:\n",
    "    pickle.dump(y, fp)"
   ]
  }
 ],
 "metadata": {
  "interpreter": {
   "hash": "7005a85ec43a6b831037bb675e384a439dc84b9bb12a3d7a0cc7bf1b3a3e3cfc"
  },
  "kernelspec": {
   "display_name": "Python 3.10.12 ('crop_env')",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.10.12"
  },
  "orig_nbformat": 4
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
