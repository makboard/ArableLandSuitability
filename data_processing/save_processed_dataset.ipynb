{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 33,
   "metadata": {},
   "outputs": [],
   "source": [
    "import os\n",
    "import pickle\n",
    "import sys\n",
    "\n",
    "import copy\n",
    "import random\n",
    "import joblib\n",
    "import numpy as np\n",
    "import pandas as pd\n",
    "import rioxarray as rxr\n",
    "import torch\n",
    "from sklearn.preprocessing import OneHotEncoder\n",
    "from sklearn.model_selection import train_test_split\n",
    "from tqdm import tqdm\n",
    "\n",
    "sys.path.append('..')\n",
    "\n",
    "from sklearn.model_selection import train_test_split\n",
    "from sklearn.preprocessing import MinMaxScaler\n",
    "\n",
    "# from src.model_utils import downsample, get_unique_values, reshape_data"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Paths to data"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 34,
   "metadata": {},
   "outputs": [],
   "source": [
    "# defining paths\n",
    "path_to_npys_data = os.path.join(\"..\", \"data\", \"npys_data\")\n",
    "\n",
    "pathTarget = os.path.join(os.path.join(path_to_npys_data, \"target_croplands.npy\"))\n",
    "pathFeatures = os.path.join(path_to_npys_data, \"features_initial_data.npy\")\n",
    "pathMorf = os.path.join(path_to_npys_data, \"features_morf_data.npy\")\n",
    "pathTarget_tif = os.path.join(\"..\", \"data\", \"target\", \"target_croplands.tif\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "##  Data"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 35,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Features\n",
    "climate_features = pd.DataFrame.from_dict(np.load(pathFeatures, allow_pickle=True), orient=\"columns\")\n",
    "morf_features = pd.DataFrame.from_dict(\n",
    "        np.load(pathMorf, allow_pickle=True), orient=\"columns\"\n",
    "    )\n",
    "\n",
    "climate_keys = list(climate_features.keys())\n",
    "morf_keys = list(morf_features.keys())\n",
    "\n",
    "with open(os.path.join(path_to_npys_data, \"climate_keys.pkl\"), 'wb') as file:\n",
    "    pickle.dump(climate_keys, file)\n",
    "    \n",
    "with open(os.path.join(path_to_npys_data, \"morf_keys.pkl\"), 'wb') as file:\n",
    "    pickle.dump(morf_keys, file)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 36,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Target Variable\n",
    "y = pd.DataFrame.from_dict(np.load(pathTarget, allow_pickle=True), orient=\"columns\")\n",
    "y = y[\"Target\"].astype(int)\n",
    "# Set classes 4,5 to 0\n",
    "y =  pd.DataFrame({\"target\": np.where(y > 3, 0, y)})\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 37,
   "metadata": {},
   "outputs": [],
   "source": [
    "# combine climate morf and target and then filter to make holdout \n",
    "data = pd.concat([climate_features.drop(columns=['latitude', 'longitude']), morf_features, y], axis=1)\n",
    "\n",
    "main_data = data[data['longitude'] <= 100]\n",
    "hold_out = data[(115 <= data['longitude']) & (data['longitude'] <= 135) &\n",
    "                     (42 <= data['latitude']) & (data['latitude'] <= 55)]\n",
    "\n",
    "with open(os.path.join(path_to_npys_data, \"hold_out.pkl\"), 'wb') as file:\n",
    "    pickle.dump(hold_out, file)\n",
    "\n",
    "del data"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 38,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Calculate the total number of samples in each class\n",
    "class_counts = np.unique(main_data['target'], return_counts=True)[1]\n",
    "\n",
    "# Calculate the total number of samples\n",
    "total_samples = sum(class_counts)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 39,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "(21819700, 165)"
      ]
     },
     "execution_count": 39,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "main_data.shape"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 40,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "2450 8906\n"
     ]
    }
   ],
   "source": [
    "# get nrows and ncols using preprocessed tif\n",
    "nrows, ncols = rxr.open_rasterio(pathTarget_tif).squeeze().where(rxr.open_rasterio(pathTarget_tif).squeeze()[\"x\"] <= 100, drop=True).shape\n",
    "print(nrows, ncols)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 41,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Reshape features and target dataframes back to its original shape\n",
    "y = main_data.pop(\"target\").to_numpy().reshape(nrows, ncols)\n",
    "X = main_data.values.reshape(nrows, ncols, -1)\n",
    "\n",
    "with open(os.path.join(path_to_npys_data, \"X_keys.pkl\"), 'wb') as file:\n",
    "    pickle.dump(X.keys(), file)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 42,
   "metadata": {},
   "outputs": [],
   "source": [
    "# 2. Function to generate 200x200 blocks from data\n",
    "def generate_blocks(data, block_size=(200, 200)):\n",
    "    blocks = []\n",
    "    n_rows, n_cols = data.shape[:2]\n",
    "    block_rows, block_cols = block_size\n",
    "\n",
    "    for i in range(0, n_rows, block_rows):\n",
    "        for j in range(0, n_cols, block_cols):\n",
    "            # Ensure that the block has the specified size\n",
    "            block = data[i:i + block_rows, j:j + block_cols]\n",
    "            blocks.append(block)\n",
    "\n",
    "    return blocks"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 95,
   "metadata": {},
   "outputs": [],
   "source": [
    "def generate_subsets(blocks, empty, max_iterations, class_counts, options, options_distr, train_portion, val_test_portion):\n",
    "    iteration = 0\n",
    "    while blocks and iteration < max_iterations:\n",
    "        random_element = blocks.pop(random.randint(0, len(blocks) - 1))\n",
    "        block_distr = {value: count for value, count in zip(*np.unique(random_element[1].flatten(), return_counts=True))}\n",
    "        block_distr = {key: block_distr.get(key, 0) for key in [0, 1, 2, 3]}\n",
    "        for key, value in empty.items():\n",
    "            block_distr[key] += value \n",
    "        \n",
    "        indexes = list(range(len(options)))\n",
    "        random.shuffle(indexes)\n",
    "        \n",
    "        for j in range(len(indexes)):\n",
    "            if not any(options_distr[indexes[j]][i] + block_distr[i] > class_counts[i] * (train_portion if indexes[j] == 0 else val_test_portion) for i in range(len(class_counts))):\n",
    "                options[indexes[j]].append(random_element)\n",
    "                for key, value in block_distr.items():\n",
    "                    options_distr[indexes[j]][key] += value \n",
    "                break\n",
    "        else:\n",
    "            blocks.append(random_element)\n",
    "        \n",
    "        iteration += 1\n",
    "    \n",
    "    return options, options_distr, blocks"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "minblocks = np.inf\n",
    "for iter in tqdm(range(100)):\n",
    "    # Generate 200x200 blocks from X and y\n",
    "    X_blocks = generate_blocks(X, block_size=(200, 200))\n",
    "    y_blocks = generate_blocks(y, block_size=(200, 200))\n",
    "    blocks = list(zip(X_blocks, y_blocks))\n",
    "\n",
    "    max_iterations = 10 * len(blocks)\n",
    "    \n",
    "    train, val, test = [], [], []\n",
    "    train_distr, val_distr, test_distr, empty = ({0: 0, 1: 0, 2: 0, 3: 0} for i in range(4))\n",
    "\n",
    "    options = [train, val, test]\n",
    "    options_distr = [train_distr, val_distr, test_distr]\n",
    "\n",
    "    options, options_distr, blocks = generate_subsets(blocks, empty, max_iterations, class_counts, options, options_distr, 0.8, 0.1)\n",
    "    \n",
    "    if len(blocks) < minblocks:\n",
    "        minblocks = len(blocks)\n",
    "        results = copy.deepcopy(options)\n",
    "        results_distr = copy.deepcopy(options_distr)\n",
    "        residual_blocks = copy.deepcopy(blocks)\n",
    "    if minblocks == 0:\n",
    "        break\n",
    "\n",
    "# work with residuals increasing limits\n",
    "max_iterations = 10 * minblocks\n",
    "options, options_distr, blocks = generate_subsets(residual_blocks, empty, max_iterations, class_counts, results, results_distr, 0.85, 0.15)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 105,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Check residuals\n",
    "for i in range(len(blocks)):\n",
    "    print(\"Block \", i)\n",
    "    print(np.unique(residual_blocks[i][1].flatten(),return_counts=True)[1])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 106,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "train [0.8004722015891051, 0.8132737018481314, 0.7990903066878018, 0.7991303442208421]\n",
      "val [0.09993501066201975, 0.11509349027584424, 0.10091530090250612, 0.10567763380216041]\n",
      "test [0.09959278774887509, 0.0716328078760243, 0.09999439240969213, 0.09519202197699757]\n"
     ]
    }
   ],
   "source": [
    "# Check results distr\n",
    "for s, set in enumerate(['train', 'val', 'test']):\n",
    "    print(set, [results_distr[s][i] / class_counts[i] for i in range(4)])"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Data Preparation \n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Target one hot encoding and Train/test split"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 16,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "X_train shape: (10228764, 162)\n",
      "X_test shape: (2898149, 162)\n",
      "X_val shape: (511439, 162)\n"
     ]
    }
   ],
   "source": [
    "y = pd.DataFrame(y, columns=[\"Target\"])\n",
    "# read data and apply one-hot encoding\n",
    "ohe = OneHotEncoder(handle_unknown=\"ignore\", sparse=False).fit(y)\n",
    "y = ohe.transform(y)\n",
    "\n",
    "# Train/test split\n",
    "X_train, X_test, y_train, y_test = train_test_split(\n",
    "    X, y, test_size=0.25, stratify=y, random_state=123\n",
    ")\n",
    "\n",
    "X_test, X_val, y_test, y_val = train_test_split(\n",
    "    X_test, y_test, test_size=0.15, stratify=y_test, random_state=123\n",
    ")\n",
    "\n",
    "# Define scaler based on whole dataset\n",
    "scaler = MinMaxScaler()\n",
    "minmax = scaler.fit(X_train)\n",
    "joblib.dump(minmax, os.path.join(path_to_npys_data, \"scaler.save\"))\n",
    "\n",
    "# Normalization using minmax scaler\n",
    "X_train = minmax.transform(X_train)\n",
    "X_test = minmax.transform(X_test)\n",
    "X_val = minmax.transform(X_val)\n",
    "\n",
    "X = dict()\n",
    "y = dict()\n",
    "\n",
    "print(\"X_train shape:\", X_train.shape)\n",
    "print(\"X_test shape:\", X_test.shape)\n",
    "print(\"X_val shape:\", X_val.shape)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 18,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "[(1, 110024), (2, 439606), (3, 879902), (0, 8799232)]"
      ]
     },
     "execution_count": 18,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# Check Classes distribution\n",
    "get_unique_values(np.argmax(y_train, 1))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 19,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Initial data:\n",
      "[(0, 8799232), (3, 879902), (2, 439606), (1, 110024)]\n",
      "Resampled data:\n",
      "[(0, 879902), (3, 879902), (2, 439606), (1, 110024)]\n",
      "Initial data:\n",
      "[(0, 439962), (3, 43995), (2, 21981), (1, 5501)]\n",
      "Resampled data:\n",
      "[(0, 43995), (3, 43995), (2, 21981), (1, 5501)]\n"
     ]
    }
   ],
   "source": [
    "# Downsampling Class 0 up to Class 2 and oversampling Class 1\n",
    "X_train, y_train = downsample(X_train, np.argmax(y_train, 1), oversampling=False)\n",
    "X_val, y_val = downsample(X_val, np.argmax(y_val, 1), oversampling=False)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 20,
   "metadata": {},
   "outputs": [],
   "source": [
    "X[\"Train\"] = X_train\n",
    "X[\"Val\"] = X_val\n",
    "X[\"Test\"] = X_test\n",
    "y[\"Train\"] = ohe.transform(pd.DataFrame(y_train))\n",
    "y[\"Val\"] = ohe.transform(pd.DataFrame(y_val))\n",
    "y[\"Test\"] = y_test"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 21,
   "metadata": {},
   "outputs": [],
   "source": [
    "# save dictionary pkl file\n",
    "with open(os.path.join(\"..\", \"data\", \"processed_files\", \"pkls\", \"X_down.pkl\"), \"wb\") as fp:\n",
    "    pickle.dump(X, fp)\n",
    "\n",
    "with open(os.path.join(\"..\", \"data\", \"processed_files\", \"pkls\", \"y_down.pkl\"), \"wb\") as fp:\n",
    "    pickle.dump(y, fp)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 22,
   "metadata": {},
   "outputs": [],
   "source": [
    "X[\"Train\"] = reshape_data(pd.DataFrame(X_train, columns=keys))\n",
    "X[\"Val\"] = reshape_data(pd.DataFrame(X_val, columns=keys))\n",
    "X[\"Test\"] = reshape_data(pd.DataFrame(X_test, columns=keys))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 24,
   "metadata": {},
   "outputs": [],
   "source": [
    "# save dictionary pkl file\n",
    "\n",
    "with open(os.path.join(\"..\", \"data\", \"processed_files\", \"pkls\", \"X_down_lstm.pkl\"), \"wb\") as fp:\n",
    "    pickle.dump(X, fp)\n",
    "\n",
    "with open(os.path.join(\"..\", \"data\", \"processed_files\", \"pkls\", \"y_down_lstm.pkl\"), \"wb\") as fp:\n",
    "    pickle.dump(y, fp)"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "wind_env",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.10.12"
  },
  "orig_nbformat": 4
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
